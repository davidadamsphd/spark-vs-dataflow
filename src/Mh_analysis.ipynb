{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CTRL_ID = 101 # Control experiment id\n",
    "EXP_ID = 102  # Experiment experiment id\n",
    "LE_MEAN_ASP_CTRL = 1.0 # Low-end products average sale price (asp) (in Control)\n",
    "LE_MEAN_ASP_EXP = 0.9 # Low-end products average sale price (asp) (in Experiment)\n",
    "HE_MEAN_ASP_CTRL = 2 # High-end products average sale price (asp) (in Control)\n",
    "HE_MEAN_ASP_EXP = 2.2 # High-end product average sale price (asp) (in Experiment)\n",
    "    \n",
    "NUM_LE = 10 # Number of low-end products\n",
    "NUM_HE = 12 # Number of high-end products\n",
    "MEAN_LE_IMPS_CTRL = 5 # Averge number of impressions for low-end products (in Control)\n",
    "MEAN_HE_IMPS_CTRL = 20 # Averge number of impressions for high-end products (in Control)\n",
    "MEAN_LE_IMPS_EXP = 8 # Averge number of impressions for low-end products (in Experiment)\n",
    "MEAN_HE_IMPS_EXP = 24 # Averge number of impressions for high-end products (in Experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "Creating fake data\n",
    "\n",
    "Letâ€™s make two categories of products: low-end and high-end (mean impression of 100, 10000) and have 100 in each category.\n",
    "Each product will have a different average ctr (sampled from a Beta Distribution:\n",
    "say Beta(3,15) -- mean of ~0.167 for both control and experiment\n",
    "\n",
    "The average sale price (asp) for are Normally distributed N(1, 0.2), N(2, 0.2), N(0.9, 0.2), N(2.2, 0.2)\n",
    "for sa-control, la-control, sa-exp, la-exp, respectively\n",
    "First, get the total number of simulated impressions per product: sample poission(\\lambda)\n",
    "'''\n",
    "\n",
    "def calc_sim_data(exp_id, product_ids, ctrs, mean_imps, mean_asp, size):\n",
    "    exp_ids = exp_id*(np.ones(size))\n",
    "    imps = np.random.poisson(mean_imps, size)\n",
    "    zeros = 0.0*(np.ones(size))\n",
    "    ones = np.ones(size)\n",
    "    data = np.concatenate(([exp_ids], [product_ids], [ones], [zeros], [ctrs]), axis=0).transpose()\n",
    "    full_data = np.repeat(data,imps, axis=0)\n",
    "\n",
    "    np.set_printoptions(suppress=True) # Supress scientific notation.\n",
    "    for r in full_data:\n",
    "        r[3] = np.random.normal(mean_asp, 0.2)\n",
    "        r[4] = np.random.binomial(1, r[4])\n",
    "    return full_data\n",
    "    \n",
    "CTRL_ID = 101\n",
    "EXP_ID = 102\n",
    "LE_MEAN_ASP_CTRL = 1.0\n",
    "LE_MEAN_ASP_EXP = 0.9\n",
    "HE_MEAN_ASP_CTRL = 2\n",
    "HE_MEAN_ASP_EXP = 2.2\n",
    "\n",
    "NUM_LE = 10\n",
    "NUM_HE = 12\n",
    "MEAN_LE_IMPS_CTRL = 5\n",
    "MEAN_HE_IMPS_CTRL = 20\n",
    "MEAN_LE_IMPS_EXP = 8\n",
    "MEAN_HE_IMPS_EXP = 24\n",
    "\n",
    "le_product_ids = np.arange(1000,1000+NUM_LE)\n",
    "he_product_ids = np.arange(10000,10000+NUM_HE)\n",
    "\n",
    "le_ctr = np.random.beta(3, 15, NUM_LE)\n",
    "he_ctr = np.random.beta(3, 15, NUM_HE)\n",
    "\n",
    "le_data_ctrl = calc_sim_data(CTRL_ID, le_product_ids, le_ctr, MEAN_LE_IMPS_CTRL, LE_MEAN_ASP_CTRL, NUM_LE)\n",
    "le_data_exp = calc_sim_data(EXP_ID, le_product_ids, le_ctr, MEAN_LE_IMPS_EXP, LE_MEAN_ASP_EXP, NUM_LE)\n",
    "he_data_ctrl = calc_sim_data(CTRL_ID, he_product_ids, he_ctr, MEAN_HE_IMPS_CTRL, HE_MEAN_ASP_CTRL, NUM_HE)\n",
    "he_data_exp = calc_sim_data(EXP_ID, he_product_ids, he_ctr, MEAN_HE_IMPS_EXP, HE_MEAN_ASP_EXP, NUM_HE)\n",
    "\n",
    "all_data = np.concatenate(([le_data_ctrl], [le_data_exp],\n",
    "                          [he_data_ctrl], [he_data_exp]), axis=1)[0]\n",
    "np.set_printoptions(suppress=True)\n",
    "np.savetxt('sim_data_{0}_{1}.csv'.format(NUM_LE, NUM_HE), all_data, fmt='%i,%i,%i,%5.2f,%i',\n",
    "           header=\"exp_id,product_id,impressions,price,clicks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidada/apps/spark-2.0.0-bin-hadoop2.7\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.10 (default, Oct 23 2015 19:19:21)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "print(spark_home)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip')) # for Spark 1.4\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.1-src.zip')) # for Spark 2.0\n",
    "\n",
    "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-896be7c1c5f0>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-896be7c1c5f0>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Calculate the numerator:\n",
    "# sum_i price_{ctrlid,i} * click_{expid,i} * w_i\n",
    "# and the denominator:\n",
    "# sum_i price_{expid,i} * click_{ctrlid,i} * w_i\n",
    "# where w_i = 1/(click_{ctrlid,i} + click{expid,i})\n",
    "def get_ctrl_exp(product_data):\n",
    "    # The shape is (product_id, iterable(id, price, clicks))\n",
    "    r = list(product_data[1])\n",
    "    exp_idx = 0\n",
    "    assert len(r) == 2\n",
    "    if int(r[0][exp_idx]) == CTRL_ID and int(r[1][exp_idx]) == EXP_ID:\n",
    "        ctrl = r[0]\n",
    "        exp = r[1]\n",
    "    elif int(r[1][exp_idx]) == CTRL_ID and int(r[0][exp_idx]) == EXP_ID:\n",
    "        ctrl = r[1]\n",
    "        exp = r[0]\n",
    "    else:\n",
    "        assert False\n",
    "    return ctrl, exp\n",
    "\n",
    "\n",
    "def calc_numerator(product_data):\n",
    "    (price_idx, click_idx) = (1, 2)\n",
    "    [ctrl, exp] = get_ctrl_exp(product_data)\n",
    "    w_inverse = (ctrl[click_idx] + exp[click_idx])\n",
    "    if w_inverse > 0:\n",
    "        return (ctrl[price_idx] * exp[click_idx]) / w_inverse\n",
    "    else:\n",
    "        return 0\n",
    "            \n",
    "def calc_denominator(product_data):\n",
    "    (price_idx, click_idx) = (1, 2)\n",
    "    [ctrl, exp] = get_ctrl_exp(product_data)\n",
    "    w_inverse = (ctrl[click_idx] + exp[click_idx])\n",
    "    if w_inverse > 0:\n",
    "        return (exp[price_idx] * ctrl[click_idx]) / w_inverse\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "(exp, prod, imps, price, clicks) = (0,1,2,3,4)\n",
    "def convert_line(l):\n",
    "    return [int(l[exp]), int(l[prod]), int(l[imps]), float(l[price]), int(l[clicks])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51.28058705475811, 55.114265835042154)\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "# We want to calculate MH(k_{a,i},n_{a,i},k_{b,i},n_{b,i}), where a and b are control and experiment\n",
    "# and there k and n in our cases are sale prices and clicks.\n",
    "input_rdd = sc.textFile('sim_data_{0}_{1}.csv'.format(NUM_LE, NUM_HE))\n",
    "header = input_rdd.first() # Remove the first line.\n",
    "parsed_input_rdd = input_rdd.filter(lambda x: x !=header).map(lambda x: convert_line(x.split(',')))\n",
    "transformed = parsed_input_rdd.map(lambda x: ((x[exp], x[prod]), (x[clicks]*x[price], x[clicks])))\n",
    "\n",
    "(sp, clks) = (0, 1) # sale price and clicks\n",
    "(ep, sc) = (0, 1) # exp_id&product_id, sp&clicks\n",
    "(exp2, prod2) = (0, 1) # exp_id, product_id\n",
    "# For each product cross exp_id, sum the sale prices and clicks\n",
    "grouped_result = transformed.reduceByKey(lambda x,y: (x[sp]+y[sp], x[clks]+y[clks]))\n",
    "grouped_by_product = grouped_result.map(lambda x: ((x[ep][prod2]), (x[ep][exp2], x[sc][sp], x[sc][clks]))).groupByKey()\n",
    "\n",
    "numerator_sum = grouped_by_product.map(lambda x: calc_numerator(x)).reduce(add)\n",
    "denominator_sum = grouped_by_product.map(lambda x: calc_denominator(x)).reduce(add)\n",
    "print(numerator_sum, denominator_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:oauth2client.contrib.multistore_file:The oauth2client.contrib.multistore_file module has been deprecated and will be removed in the next release of oauth2client. Please migrate to multiprocess_file_storage.\n",
      "ERROR:root:Error while visiting split line\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'convert_line' is not defined [while running 'split line']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-566ad293fa37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m  | df.CombineGlobally('denom', sum))\n\u001b[1;32m     38\u001b[0m \u001b[0mdenominator_sum\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'save denominator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextFileSink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./denominator_sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/pipeline.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/runners/direct_runner.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, pipeline)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDirectPipelineRunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final: Debug counters: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_counters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     return DirectPipelineResult(state=PipelineState.DONE,\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/runners/runner.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, pipeline)\u001b[0m\n\u001b[1;32m     94\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunVisitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/pipeline.pyc\u001b[0m in \u001b[0;36mvisit\u001b[0;34m(self, visitor)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mvisited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvalueish\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/pipeline.pyc\u001b[0m in \u001b[0;36mvisit\u001b[0;34m(self, visitor, pipeline, visited)\u001b[0m\n\u001b[1;32m    414\u001b[0m       \u001b[0mvisitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_composite_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m       \u001b[0mvisitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleave_composite_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/pipeline.pyc\u001b[0m in \u001b[0;36mvisit\u001b[0;34m(self, visitor, pipeline, visited)\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0mvisitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleave_composite_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m       \u001b[0mvisitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;31m# Visit the outputs (one or more). It is essential to mark as visited the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/runners/runner.pyc\u001b[0m in \u001b[0;36mvisit_transform\u001b[0;34m(self, transform_node)\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mvisit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m           \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error while visiting %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/runners/runner.pyc\u001b[0m in \u001b[0;36mrun_transform\u001b[0;34m(self, transform_node)\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'run_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    168\u001b[0m         'Execution of [%s] not implemented in runner %s.' % (\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/runners/direct_runner.pyc\u001b[0m in \u001b[0;36mfunc_wrapper\u001b[0;34m(self, pvalue, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/runners/direct_runner.pyc\u001b[0m in \u001b[0;36mrun_ParDo\u001b[0;34m(self, transform_node)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/runners/common.pyc\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdofn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise_augmented\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreraise_augmented\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/runners/common.pyc\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPerThreadLoggingContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdofn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise_augmented\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/typehints/typecheck.pyc\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, context, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdofn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_check_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/typehints/typecheck.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, method, context, args, kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeCheckError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m       error_msg = ('Runtime type violation detected within ParDo(%s): '\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/transforms/core.pyc\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, context, *args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_argspec_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidada/dev/MH/spark-vs-dataflow/venv/lib/python2.7/site-packages/google/cloud/dataflow/transforms/core.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;34m'Received %r instead for %s argument.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         % (fn, 'first' if label is None else 'second'))\n\u001b[0;32m--> 675\u001b[0;31m   \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[0;31m# Proxy the type-hint information from the original function to this new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-566ad293fa37>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m  \u001b[0;34m|\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load records'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextFileSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sim_data_{0}_{1}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_LE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_HE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m  \u001b[0;34m|\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filter header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'#'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m  | df.Map('split line', lambda x: convert_line(x.split(','))))\n\u001b[0m\u001b[1;32m     16\u001b[0m transformed = (parsed_input_rdd\n\u001b[1;32m     17\u001b[0m  | df.Map((lambda x: ((x[exp], x[prod]), (x[price]*x[clicks], x[clicks])))))\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'convert_line' is not defined [while running 'split line']"
     ]
    }
   ],
   "source": [
    "import google.cloud.dataflow as df\n",
    "\n",
    "def t_sum(values):\n",
    "    result = [0,0]\n",
    "    for v in values:\n",
    "        result[0] += v[0]\n",
    "        result[1] += v[1]\n",
    "    return (result[0], result[1])\n",
    "\n",
    "# Create a pipeline executing on a direct runner (local, non-cloud).\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "parsed_input_rdd = (p\n",
    " | df.Read('load records', df.io.TextFileSource('sim_data_{0}_{1}.csv'.format(NUM_LE, NUM_HE)))\n",
    " | df.Filter('filter header', lambda x: x[0] != '#')\n",
    " | df.Map('split line', lambda x: convert_line(x.split(','))))\n",
    "transformed = (parsed_input_rdd\n",
    " | df.Map((lambda x: ((x[exp], x[prod]), (x[price]*x[clicks], x[clicks])))))\n",
    "\n",
    "(sp, clks) = (0, 1) # sale price and clicks\n",
    "(ep, sc) = (0, 1) # exp_id&product_id, sp&clicks\n",
    "(exp2, prod2) = (0, 1) # exp_id, product_id\n",
    "\n",
    "# For each product cross exp_id, sum the sale prices and clicks\n",
    "grouped_result = (transformed\n",
    " | df.CombinePerKey('combine per product/id', t_sum))\n",
    "grouped_by_product = (grouped_result\n",
    " | df.Map(lambda x: ((x[ep][prod2]), (x[ep][exp2], x[sc][sp], x[sc][clks])))\n",
    " | df.GroupByKey())\n",
    "\n",
    "numerator_sum = (grouped_by_product\n",
    " | df.Map(lambda x: calc_numerator(x))\n",
    " | df.CombineGlobally('num', sum))\n",
    "numerator_sum | df.Write('save numerator', df.io.TextFileSink('./numerator_sum'))\n",
    "\n",
    "denominator_sum = (grouped_by_product\n",
    " | df.Map(lambda x: calc_denominator(x))\n",
    " | df.CombineGlobally('denom', sum))\n",
    "denominator_sum | df.Write('save denominator', df.io.TextFileSink('./denominator_sum'))\n",
    "p.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
