{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CTRL_ID = 101\n",
    "EXP_ID = 102\n",
    "SA_MEAN_CPC_CTRL = 1.0\n",
    "SA_MEAN_CPC_EXP = 0.9\n",
    "LA_MEAN_CPC_CTRL = 2\n",
    "LA_MEAN_CPC_EXP = 2.2\n",
    "\n",
    "NUM_SA = 10\n",
    "NUM_LA = 12\n",
    "MEAN_SA_IMPS_CTRL = 5\n",
    "MEAN_LA_IMPS_CTRL = 20\n",
    "MEAN_SA_IMPS_EXP = 8\n",
    "MEAN_LA_IMPS_EXP = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "Creating fake data\n",
    "\n",
    "Letâ€™s make two categories of advertisers: small and large (mean impression of 100, 10000) and have 100 in each category.\n",
    "Each advertiser will have a different average ctr (sampled from a Beta Distribution:\n",
    "say Beta(3,15) -- mean of ~0.167 for both control and experiment\n",
    "\n",
    "The CPCs for are Normally distributed N(1, 0.2), N(2, 0.2), N(0.9, 0.2), N(2.2, 0.2)\n",
    "for sa-control, la-control, sa-exp, la-exp, respectively\n",
    "First, get the total number of simulated impressions per advertiser: sample poission(\\lambda)\n",
    "'''\n",
    "\n",
    "def calc_sim_data(exp_id, adv_ids, ctrs, mean_imps, mean_cpc, size):\n",
    "    exp_ids = exp_id*(np.ones(size))\n",
    "    imps = np.random.poisson(mean_imps, size)\n",
    "    zeros = 0.0*(np.ones(size))\n",
    "    ones = np.ones(size)\n",
    "    data = np.concatenate(([exp_ids], [adv_ids], [ones], [zeros], [ctrs]), axis=0).transpose()\n",
    "    full_data = np.repeat(data,imps, axis=0)\n",
    "\n",
    "    np.set_printoptions(suppress=True) # Supress scientific notation.\n",
    "    for r in full_data:\n",
    "        r[3] = np.random.normal(mean_cpc, 0.2)\n",
    "        r[4] = np.random.binomial(1, r[4])\n",
    "    return full_data\n",
    "    \n",
    "CTRL_ID = 101\n",
    "EXP_ID = 102\n",
    "SA_MEAN_CPC_CTRL = 1.0\n",
    "SA_MEAN_CPC_EXP = 0.9\n",
    "LA_MEAN_CPC_CTRL = 2\n",
    "LA_MEAN_CPC_EXP = 2.2\n",
    "\n",
    "NUM_SA = 10\n",
    "NUM_LA = 12\n",
    "MEAN_SA_IMPS_CTRL = 5\n",
    "MEAN_LA_IMPS_CTRL = 20\n",
    "MEAN_SA_IMPS_EXP = 8\n",
    "MEAN_LA_IMPS_EXP = 24\n",
    "\n",
    "sa_adv_ids = np.arange(1000,1000+NUM_SA)\n",
    "la_adv_ids = np.arange(10000,10000+NUM_LA)\n",
    "\n",
    "sa_ctr = np.random.beta(3, 15, NUM_SA)\n",
    "la_ctr = np.random.beta(3, 15, NUM_LA)\n",
    "\n",
    "sa_data_ctrl = calc_sim_data(CTRL_ID, sa_adv_ids, sa_ctr, MEAN_SA_IMPS_CTRL, SA_MEAN_CPC_CTRL, NUM_SA)\n",
    "sa_data_exp = calc_sim_data(EXP_ID, sa_adv_ids, sa_ctr, MEAN_SA_IMPS_EXP, SA_MEAN_CPC_EXP, NUM_SA)\n",
    "la_data_ctrl = calc_sim_data(CTRL_ID, la_adv_ids, la_ctr, MEAN_LA_IMPS_CTRL, LA_MEAN_CPC_CTRL, NUM_LA)\n",
    "la_data_exp = calc_sim_data(EXP_ID, la_adv_ids, la_ctr, MEAN_LA_IMPS_EXP, LA_MEAN_CPC_EXP, NUM_LA)\n",
    "\n",
    "all_data = np.concatenate(([sa_data_ctrl], [sa_data_exp],\n",
    "                          [la_data_ctrl], [la_data_exp]), axis=1)[0] # why is it adding the extra layer???\n",
    "#print(sa_data)\n",
    "np.set_printoptions(suppress=True)\n",
    "np.savetxt('sim_data_{0}_{1}.csv'.format(NUM_SA, NUM_LA), all_data, fmt='%i,%i,%i,%5.2f,%i',\n",
    "           header=\"exp_id,advertiser_id,impressions,cost,clicks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidada/apps/spark-1.4.1-bin-hadoop2.6\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at <ipython-input-31-dcb2159c5100>:9 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-dcb2159c5100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python/lib/py4j-0.8.2.1-src.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mexecfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python/pyspark/shell.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/davidada/apps/spark-1.4.1-bin-hadoop2.6/python/pyspark/shell.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSystemProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.executor.uri\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SPARK_EXECUTOR_URI\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"PySparkShell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyFiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidada/apps/spark-1.4.1-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/Users/davidada/apps/spark-1.4.1-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    243\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 245\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at <ipython-input-31-dcb2159c5100>:9 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "print(spark_home)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "\n",
    "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the numerator:\n",
    "# sum_i cost_{ctrlid,i} * click_{expid,i} * w_i\n",
    "# and the denominator:\n",
    "# sum_i cost_{expid,i} * click_{ctrlid,i} * w_i\n",
    "# where w_i = 1/(click_{ctrlid,i} + click{expid,i})\n",
    "def get_ctrl_exp(adv_data):\n",
    "    # The shape is (advertiser_id, iterable(id, cost, clicks))\n",
    "    r = list(adv_data[1])\n",
    "    assert len(r) == 2\n",
    "    if int(r[0][0]) == CTRL_ID and int(r[1][0]) == EXP_ID:\n",
    "        ctrl = r[0]\n",
    "        exp = r[1]\n",
    "    elif int(r[1][0]) == CTRL_ID and int(r[0][0]) == EXP_ID:\n",
    "        ctrl = r[1]\n",
    "        exp = r[0]\n",
    "    else:\n",
    "        assert False\n",
    "    return ctrl, exp\n",
    "\n",
    "def calc_numerator(adv_data):\n",
    "    [ctrl, exp] = get_ctrl_exp(adv_data)\n",
    "    w_inverse = (ctrl[2] + exp[2])\n",
    "    if w_inverse > 0:\n",
    "        return (ctrl[1] * exp[2] / (ctrl[2] + exp[2]))\n",
    "    else:\n",
    "        return 0\n",
    "            \n",
    "def calc_denominator(adv_data):\n",
    "    [ctrl, exp] = get_ctrl_exp(adv_data)\n",
    "    w_inverse = (ctrl[2] + exp[2])\n",
    "    if w_inverse > 0:\n",
    "        return (exp[1] * ctrl[2] / (ctrl[2] + exp[2]))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def convert_line(l):\n",
    "    return [int(l[0]), int(l[1]), int(l[2]), float(l[3]), int(l[4])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49.92499707602339, 53.84168922305764)\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "# We want to calculate MH(k_{a,i},n_{a,i},k_{b,i},n_{b,i}), where a and b are control and experiment\n",
    "# and there k and n in our cases are cost and clicks.\n",
    "input_rdd = sc.textFile('sim_data_{0}_{1}.csv'.format(NUM_SA, NUM_LA))\n",
    "header = input_rdd.first() # Remove the first line.\n",
    "parsed_input_rdd = input_rdd.filter(lambda x: x !=header).map(lambda x: convert_line(x.split(',')))\n",
    "transformed = parsed_input_rdd.map(lambda x: ((x[0], x[1]), (x[3]*x[4], x[4])))\n",
    "\n",
    "# For each advertiser cross exp_id, sum the cost and clicks\n",
    "grouped_result = transformed.reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "grouped_by_advertiser = grouped_result.map(lambda x: ((x[0][1]), (x[0][0], x[1][0], x[1][1]))).groupByKey()\n",
    "\n",
    "numerator_sum = grouped_by_advertiser.map(lambda x: calc_numerator(x)).reduce(add)\n",
    "denominator_sum = grouped_by_advertiser.map(lambda x: calc_denominator(x)).reduce(add)\n",
    "print(numerator_sum, denominator_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# exp_id,advertiser_id,impressions,cpc,clicks\\n', '101,1000,1, 0.97,0\\n']\n"
     ]
    }
   ],
   "source": [
    "f = open('sim_data_{0}_{1}.csv'.format(NUM_SA, NUM_LA))\n",
    "print(f.readlines()[0:2])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.runner.PipelineResult at 0x1072a9050>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.cloud.dataflow as df\n",
    "\n",
    "def t_sum(values):\n",
    "    result = [0,0]\n",
    "    for v in values:\n",
    "        result[0] += v[0]\n",
    "        result[1] += v[1]\n",
    "    return (result[0], result[1])\n",
    "\n",
    "# Create a pipeline executing on a direct runner (local, non-cloud).\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "parsed_input_rdd = (p\n",
    " | df.Read('load records', df.io.TextFileSource('sim_data_{0}_{1}.csv'.format(NUM_SA, NUM_LA)))\n",
    " | df.Filter('filter header', lambda x: x[0] != '#')\n",
    " | df.Map('split line', lambda x: convert_line(x.split(','))))\n",
    "transformed = (parsed_input_rdd\n",
    " | df.Map((lambda x: ((x[0], x[1]), (x[3]*x[4], x[4])))))\n",
    "\n",
    "# For each advertiser cross exp_id, sum the cost and clicks\n",
    "grouped_result = (transformed\n",
    " | df.CombinePerKey('combine per adv/id', t_sum))\n",
    "grouped_by_advertiser = (grouped_result\n",
    " | df.Map(lambda x: ((x[0][1]), (x[0][0], x[1][0], x[1][1])))\n",
    " | df.GroupByKey())\n",
    "\n",
    "numerator_sum = (grouped_by_advertiser\n",
    " | df.Map(lambda x: calc_numerator(x))\n",
    " | df.CombineGlobally('num', sum))\n",
    "numerator_sum | df.Write('save numerator', df.io.TextFileSink('./numerator_sum'))\n",
    "\n",
    "denominator_sum = (grouped_by_advertiser\n",
    " | df.Map(lambda x: calc_denominator(x))\n",
    " | df.CombineGlobally('denom', sum))\n",
    "denominator_sum | df.Write('save denominator', df.io.TextFileSink('./denominator_sum'))\n",
    "p.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
